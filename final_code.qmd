---
title: "final_code"
author: "Thomas Adams & Keegan Brown"
format: 
  html:
    self-contained: true
editor: visual
editor_options: 
  chunk_output_type: console
---

#Note this is for stretch exercise 7 submission

#Project Description This project was inspired by the revelations of rental market collusion through Yieldstar. Rental market collusion poses serious risks for low and middle income families, as purchasing homes outright has become increasingly costly. Our goal is to identify cases where the value of a particular housing arrangement is out of line with the underlying characteristics, suggesting price inflation and possible rental collusion. The goal of this analysis is to develop a predictive model that will identify geographic areas with rents above what is considered fair market.

The data sets that will be used for this is the 2021 IPUMS ACS 1-year survey and the HUD Fair Market Rent data from 2021, which are used to calculate the payment standards for Housing Choice Voucher programs, among other housing assistance policies. The variable of interest is going to be predicted rental prices. Variables used to evaluate this include: Ownership length, group quarters status, metro status, population density within the PUMA, PUMA (public use microdata area), Census region and division, House Value, Vacancy Status, building year, bedroom count, bathroom count, and household income.

###loading libraries

```{r}
library(tidyverse)
library(ipumsr)
library(tidymodels)
library(sf)
library(readxl)
library(tigris)
library(haven)
library(foreach)
library(patchwork)
```

###Loading Raw Data

```{r}
ddi <- read_ipums_ddi("usa_00002.xml")
data <- read_ipums_micro(ddi)
usshape <- read_ipums_sf("ipums_puma_2010")
fh_data <- read_xlsx("SAFMRS21.xlsx")
```

###cleaning data prior to analysis

```{r}
#Converting label data to panacea variable types 
#removing variables that are unnecessary 
data <- data%>%
  select(!YEAR)

data%>%
  ipums_val_labels(SAMPLE)

#sample appears irrelevant, we already know its all of 2021 1 year ACS

data <- data%>%
  select(!SAMPLE)

#cluster and strata is also unnecessary because we are not using the taylor series linear approx
data <- data%>%
  select(!CLUSTER)

data <- data%>%
  select(!STRATA)

#PROPTX99 not included with rental data, not relevant 
data <- data%>%
  select(!PROPTX99)


#above did not cycle through all of them and report individually - moving to indivudal proction
data%>%
  ipums_val_labels(KITCHEN)

data%>%
  ipums_val_labels(HHTYPE)

data%>%
  ipums_val_labels(HHINCOME)

data%>%
  ipums_val_labels(HHTYPE)

data%>%
  ipums_val_labels(BUILTYR2)

data%>%
  ipums_val_labels(STATEFIP)

data%>%
  ipums_val_labels(GQ)

data%>%
  ipums_val_labels(OWNERSHP)

data%>%
  ipums_val_labels(OWNERSHPD)

data%>%
  ipums_val_labels(RENTGRS)

data%>%
  ipums_val_labels(RENT)

data%>%
ipums_var_info(RENT)

data%>%
  ipums_val_labels(VALUEH)

data%>%
  ipums_val_labels(FOODSTMP)

data%>%
  ipums_val_labels(VACANCY)

data%>%
  ipums_val_labels(BEDROOMS)

#all of the above can be converted into factor variables directly 

data <- data%>%
  mutate(HHTYPE = as.factor(HHTYPE),
         REGION = as.factor(REGION),
         STATEFIP = as.factor(STATEFIP),
         METRO = as.factor(METRO),
         GQ = as.factor(GQ),
         OWNERSHP = as.factor(OWNERSHP), 
         OWNERSHPD = as.factor(OWNERSHPD), 
         FOODSTMP = as.factor(FOODSTMP),
         VALUEH = as.factor(VALUEH), 
         VACANCY = as.factor(VACANCY), 
         KITCHEN = as.factor(KITCHEN),
         BUILTYR2 = as.factor(BUILTYR2),
         BEDROOMS = as.factor(BEDROOMS),
         COUNTYFIP = as.factor(COUNTYFIP)
  )


##because we are just interested in rental markets, all nonrental observations are dropped - also those coded with N/A or no cash rent dropped. 
data <- data%>%
  filter(OWNERSHP == 2 & RENT > 1)

data%>%
  group_by(VACANCY)%>%
  summarize()

```

#Step 1: Splitting and setting up

###Splitting data

```{r}
set.seed(9940)
acs_split <- initial_split(data, prop = 8/10)
train_data <- training(acs_split)
test_data <- testing(acs_split)

```

###EDA and Evaluating appropriate model

Because we are looking at predicting cases of rent that are above market rate, we are going to leverage a predictive model. For this exercise, we will evaluate three potential models to use: Lasso, Random forests, and Multiple-Linear regression.

```{r}
#gathering general stats on the variables 
ipums_train2 <- train_data %>%
  group_by(PUMA, STATEFIP) %>%
  summarize(mean_rent = mean(RENT))
ipums_map <- ipums_shape_inner_join(ipums_train2, usshape, by = c("PUMA", "STATEFIP"))
ipums_map <- ipums_map %>%
  filter(State != "Alaska") %>%
  filter(State != "Hawaii")


```

Our first evaluation is to look at the dispersion of rent across different markets. The map below shows the average rents by state. However, for our case we do not want variations in state law to influence our predictions and our model evaluation. Consequently, we will focus our efforts on Texas going forward.

```{r}
ggplot(data = ipums_map, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "red", high = "green")
```

```{r}
ipums_tex <- ipums_map %>%
  filter(State == "Texas")


ggplot(data = ipums_tex, aes(fill = mean_rent)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")+
  labs(title = "Mean Rent By County")+
  guides(fill=guide_legend(title="Mean Rent"))
```

For Texas, we are interested in visualizing how our variables drive market rates. Location is a prime area of dispersion. Each of the dots below represent different counties in the state of Texas. As evidenced by the dispersion across points.

```{r}

ipumstx_mr <- train_data%>%
  filter(STATEFIP == 48)%>%
  group_by(COUNTYFIP)%>%
  summarize(mean = mean(RENT))%>%
  print(n=39)

plot3 <- ggplot(data = ipumstx_mr, aes(x = COUNTYFIP, y = mean))+
      geom_point()+
      labs(title = "Mean Rent by County")+
      xlab("County Code")+
      ylab("Mean Rent")+
      theme_minimal()
  
plot3
```

Adding additional information into the model begins to normalize the relationship. Here we, combine the effects of the PUMA with the year that the building was created. 

```{r}
ipumstx_pyr <- train_data %>%
  filter(STATEFIP == 48) %>%
  group_by(PUMA) %>%
  mutate(mean = mean(RENT))
  

  lm(mean ~ PUMA + BUILTYR2, data = ipumstx_pyr) %>%
  summary()
##Here we see that our PUMA variable is highly significant, and most year variables are as well. 

usshape_tx <- usshape%>%
  filter(STATEFIP == 48)
  
ipums_map2 <- ipums_shape_inner_join(ipumstx_pyr, usshape_tx, by = "PUMA")

ipumstx_map2 <- train_data %>%
  filter(COUNTYFIP == 17) %>%
  group_by(PUMA) %>%
  mutate(mean = mean(RENT))%>%
  ungroup()

ipumstx_map2 <- ipums_shape_inner_join(ipumstx_map2, usshape_tx, by = "PUMA")

##here we are doing a geo-spacial demonstration of the cross mapping of mean_rent, density, and build year for Austin. 

## shifting from factor to numeric - possible because it is based on build year but removing irrelevant data. You can also see how gerrymande4red the county is. 
ipumstx_map2 <- ipumstx_map2%>%
  mutate(BUILTYR2 = as.numeric(BUILTYR2))%>%
  group_by(BUILTYR2)%>%
  filter(BUILTYR2 != 0)%>%
  ungroup()

plot4 <- ipumstx_map2%>%
  group_by(PUMA)%>%
  ggplot(aes(fill = BUILTYR2)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")


plot5<- ggplot(data = ipumstx_map2, aes(fill = DENSITY)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")

plot6 <- ggplot(data = ipumstx_map2, aes(fill = mean)) +
  geom_sf() +
  scale_fill_gradient(low = "green", high = "red")
  
plot4 + plot5 + plot6

```

For the error metric in this model we will be using RMSE. RMSE is a good error metric for all three candidate models we are considering (RF/MLR/Lasso). This metric will also give us the error in terms of the rental price expected vs. actual rate, enabling easy interpretation and evaluation of the model efficacy. Rule of thumb suggests an RMSE between .2 and .5 provide reasonable predictive power in a normalized model. This would evaluation will change based on the mean rent level and standard deviation, but consistent prediction within a standard deviation would be strong. 

#Step 2: Models
To better manage for computational constraints, we are moving explcitly to texas only. Moving back to our variables of interest, there is some preprocessing required for a few of our variables. For example, there are some ordinal variables of interest and nominal variables of interest that have already been preprocessed and turned into factor variables. Finally, we already did some preprocessing by removing variables from our dataset that were not providing descriptive value, or that didn't add any support to rental based analysis. 

We will start with preparing our data. For our model specifications, we are going to use the following variables: Year built, bedroom count, density, and PUMA. One iteration that is also possible is the percent of vacancies in a given PUMA. The third will include metro status. 
We are going to standardize our density variable, which is continuous. We are not going to standardize our outcome variable because it provides key descriptive value. This will be done through step_normalize. 

```{r}

#creating the vacancy percentage variable by PUMA
train_data %>%
  group_by(PUMA) %>%
  summarise(vac_per = sum(VACANCY %in% c(1, 3, 9)) / sum(n= n()))%>%
  print(n=100)




```




#Next, we prepare our density variable by standardizing 


##We now are going to prep our data for resampling methods? 


```{r}






```

```{r}

```

```{r}

```
